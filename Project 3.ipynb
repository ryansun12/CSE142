{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 for CSE142\n",
    "## CIFAR-10 Classification\n",
    "\n",
    "Importing necessary packages:\n",
    "\n",
    "- tensorflow/keras: API to build and train models\n",
    "- matplotlib: generate graphs\n",
    "- numpy: perform array operations\n",
    "- pandas: to work with dataframes\n",
    "- pickle: for serializing and de-serializing a Python object\n",
    "- os: for file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import pickle\n",
    "# import os, os.path\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Initialize relevant variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv(\"Semi1_Labels.csv\") #change this for different versions\n",
    "train_path = 'Train_Image' \n",
    "test_path = 'Test_Image' \n",
    "labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "#labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_image(img_number):\n",
    "    img = mpimg.imread(train_path + f\"/{img_number}.png\") \n",
    "    return img\n",
    "def get_test_image(img_number):\n",
    "    img = mpimg.imread(test_path + f\"/{img_number}.png\") \n",
    "    return img\n",
    "def get_label(arr):\n",
    "    max_index_col = np.argmax(arr, axis=0)\n",
    "    return labels[max_index_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.imshow(get_image(7777))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.imshow(get_test_image(999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function that prints image info**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_img_info(img):\n",
    "    rows,cols,channels = img.shape \n",
    "    img_size = rows*cols*channels # channels means rgb colors i think, so should always be 3?\n",
    "    img_to_1D = img.reshape(img_size) #numPy reshape\n",
    "    print(\"rows, cols, channels:\", rows, cols, channels)\n",
    "    print(\"image size:\", img_size)\n",
    "#     print(\"image original:\", img)\n",
    "#     print(\"image reshape:\", img_to_1D)\n",
    "#     return img_to_1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_img_info(get_image(3)) \n",
    "print(get_image(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the  Data\n",
    "\n",
    "We need to separate the labeled data from the unlabeled\n",
    "\n",
    "Also need to encode labels as numbers instead of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#is_unlabelled = labels_df[\"Train Label\"] == NAN\n",
    "labelled = (labels_df[\"Train Label\"]!= \"NAN\")\n",
    "unlabelled = (labels_df[\"Train Label\"] == \"NAN\")\n",
    "\n",
    "unlabelled_data = (labels_df[unlabelled])\n",
    "# print(unlabelled_data)\n",
    "\n",
    "labelled_data = (labels_df[labelled])\n",
    "# print(labelled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labeled = []\n",
    "unlabeled = []\n",
    "\n",
    "labeled = labelled_data[\"Train Label\"].tolist()\n",
    "unlabeled = unlabelled_data[\"Train Label\"].tolist()\n",
    "\n",
    "# print (labeled) \n",
    "# print (unlabeled) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#col = labels_df.columns\n",
    "#for i, entry in enumerate(labels_df[col[0]]):\n",
    " #   if (entry == 'NAN'):\n",
    "  #      unlabeled.append(i)\n",
    "   # else:\n",
    "   #     labeled.append([i, entry])\n",
    "# print(\"labeled\", labeled)\n",
    "# print(\"unlabeled\", unlabeled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Need to reshape training data and labels\n",
    "train_images, train_labels, test_images, test_labels = [], [], [], []\n",
    "for i in labelled_data.iterrows():\n",
    "    train_images.append(get_image(i[0]))\n",
    "    train_labels.append(i[1]['Train Label'])\n",
    "print(len(train_images))\n",
    "for i in range(36):\n",
    "    plt.subplot(6,6,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i])\n",
    "    plt.xlabel(train_labels[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(train_labels)\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### USED FOLLOWING TUTORIAL: https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/#:~:text=A%20one%20hot%20encoding%20allows,output%20variables%20that%20are%20categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "##One hot \n",
    "# print(train_labels)\n",
    "values = array(train_labels)\n",
    "copy = values\n",
    "print(values)\n",
    "\n",
    "def encode(vals):\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(vals) \n",
    "    onehot_encoder = OneHotEncoder(sparse =False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    return onehot_encoded\n",
    "\n",
    "onehot_encoded = encode(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(onehot_encoded)\n",
    "# print(len(onehot_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#change this to one-hot\n",
    "# for i in range(len(train_labels)):\n",
    "#     for j in range(len(labels)):\n",
    "#         if train_labels[i] == labels[j]:\n",
    "#             train_labels[i] = j\n",
    "# print (train_labels)\n",
    "\n",
    "\n",
    "train_labels = onehot_encoded\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#split labeled data so that we have some labeled testing data 80/20\n",
    "test_images =  np.array(train_images[1600:])\n",
    "train_images = np.array(train_images[:1600])\n",
    "test_labels = np.array(train_labels[1600:])\n",
    "train_labels = np.array(train_labels[:1600])\n",
    "print(len(train_images), len(train_labels), len(test_images), len(test_labels))\n",
    "\n",
    "# print(train_images, test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback1 = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "callback2 = tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0, patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "### Convolutional Neural Network\n",
    "\n",
    "As input, a CNN takes tensors of shape (height, width, channels) = (32, 32, 3)\n",
    "\n",
    "This is based on https://www.tensorflow.org/tutorials/images/cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model = models.Sequential()\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu',kernel_initializer='he_uniform', padding='same',input_shape=(32, 32, 3)))\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform',padding='same'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu',kernel_initializer='he_uniform', padding='same'))\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu',kernel_initializer='he_uniform', padding='same'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu',kernel_initializer='he_uniform', padding='same'))\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu',kernel_initializer='he_uniform', padding='same'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Flatten())\n",
    "# model.add(layers.Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "# model.add(layers.Dense(10, activation='softmax'))\n",
    "# model.summary()\n",
    "\n",
    "# model = models.Sequential()\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(32, 32, 3)))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "# # model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Flatten())\n",
    "# # model.add(layers.Dense(128, activation='relu'))\n",
    "# model.add(layers.Dense(64, activation='relu'))\n",
    "# model.add(layers.Dense(10, activation='softmax'))\n",
    "# model.summary()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform', input_shape=(32, 32, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "# model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe try Stochastic Gradient Descent optimizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_images, train_labels, epochs=100, \n",
    "                    validation_data=(test_images, test_labels), callbacks=[callback1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting the Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label some of our unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_labels = [];\n",
    "new_images = [];\n",
    "\n",
    "for j,i in enumerate(unlabelled_data.iterrows()):\n",
    "    if j < 500:\n",
    "        new_images.append(get_image(i[0]))\n",
    "#         print(i[0])\n",
    "\n",
    "\n",
    "predictions = model.predict(np.array(new_images))\n",
    "for i in predictions:\n",
    "    new_labels.append(get_label(i))\n",
    "    \n",
    "print(len(new_images), len(new_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.grid(False)\n",
    "    plt.imshow(new_images[i])\n",
    "    plt.xlabel(new_labels[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hot encode predicted labels and concatenate with the original labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_train_images = np.concatenate((np.array(new_images), train_images), axis=0)\n",
    "\n",
    "#copy is original train_labels, n=2000, not encoded\n",
    "new_train_labels = np.concatenate((new_labels, copy), axis=0)\n",
    "new_train_labels = encode(new_train_labels)\n",
    "\n",
    "train_labels = np.array(new_train_labels[:2100])\n",
    "print(len(new_train_images),len(train_labels))\n",
    "print(len(test_labels), len(test_images))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_model = models.Sequential()\n",
    "new_model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform', input_shape=(32, 32, 3)))\n",
    "new_model.add(layers.BatchNormalization())\n",
    "new_model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
    "new_model.add(layers.BatchNormalization())\n",
    "new_model.add(layers.MaxPooling2D((2, 2)))\n",
    "new_model.add(layers.Dropout(0.2))\n",
    "new_model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
    "new_model.add(layers.BatchNormalization())\n",
    "new_model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
    "new_model.add(layers.MaxPooling2D((2, 2)))\n",
    "new_model.add(layers.Dropout(0.3))\n",
    "new_model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
    "new_model.add(layers.BatchNormalization())\n",
    "new_model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_uniform'))\n",
    "new_model.add(layers.BatchNormalization())\n",
    "new_model.add(layers.MaxPooling2D((2, 2)))\n",
    "new_model.add(layers.Dropout(0.4))\n",
    "new_model.add(layers.Flatten())\n",
    "new_model.add(layers.Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "new_model.add(layers.BatchNormalization())\n",
    "new_model.add(layers.Dropout(0.5))\n",
    "new_model.add(layers.Dense(10, activation='softmax'))\n",
    "new_model.summary()\n",
    "\n",
    "new_model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history2 = new_model.fit(new_train_images, train_labels, epochs=100, \n",
    "                    validation_data=(test_images, test_labels), callbacks=[callback1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline model with augmented data predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tests = []\n",
    "for i in range(25):\n",
    "    tests.append(get_test_image(i))\n",
    "\n",
    "predictions = new_model.predict(np.array(tests))\n",
    "    \n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.grid(False)\n",
    "    plt.imshow(tests[i])\n",
    "    plt.xlabel(get_label(predictions[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet 50 \n",
    "\n",
    "As per TA recommendation, try a much more complex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels, test_images, test_labels = [], [], [], []\n",
    "for i in labelled_data.iterrows():\n",
    "    train_images.append(get_image(i[0]))\n",
    "    train_labels.append(i[1]['Train Label'])\n",
    "# print(len(train_images))\n",
    "\n",
    "values = array(train_labels)\n",
    "\n",
    "onehot_encoded = encode(values)\n",
    "train_labels = onehot_encoded\n",
    "test_images =  np.array(train_images[1600:])\n",
    "train_images = np.array(train_images[:1600])\n",
    "test_labels = np.array(train_labels[1600:])\n",
    "train_labels = np.array(train_labels[:1600])\n",
    "\n",
    "print(len(train_labels),len(train_images),len(test_labels),len(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# resnet = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(128,128,3), pooling='max')\n",
    "resnet = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(256,256,3))\n",
    "resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resnet_model = tf.keras.models.Sequential()\n",
    "# resnet_model.add(tf.keras.layers.Conv2DTranspose(3, (3, 3), strides=2, padding='same', activation='relu', input_shape=(32,32,3)))\n",
    "# resnet_model.add(tf.keras.layers.BatchNormalization())\n",
    "# resnet_model.add(tf.keras.layers.Conv2DTranspose(3, (3, 3), strides=2, padding='same', activation='relu'))\n",
    "# resnet_model.add(tf.keras.layers.BatchNormalization())\n",
    "resnet_model.add(layers.UpSampling2D((2,2), input_shape=(32,32,3)))\n",
    "resnet_model.add(layers.UpSampling2D((2,2)))\n",
    "resnet_model.add(layers.UpSampling2D((2,2)))\n",
    "resnet_model.add(resnet)\n",
    "resnet_model.add(layers.Flatten())\n",
    "resnet_model.add(layers.BatchNormalization())\n",
    "resnet_model.add(layers.Dense(128, activation='relu'))\n",
    "resnet_model.add(layers.Dropout(0.5))\n",
    "resnet_model.add(layers.BatchNormalization())\n",
    "resnet_model.add(layers.Dense(64, activation='relu'))\n",
    "resnet_model.add(layers.Dropout(0.5))\n",
    "resnet_model.add(layers.BatchNormalization())\n",
    "resnet_model.add(layers.Dense(10, activation='softmax'))\n",
    "resnet_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maybe try rmsprop optimizer, lr = 0.00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# resnet_model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "#               loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# resnet_model.compile(optimizer=optimizers.RMSprop(lr=2e-5), loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "resnet_model.compile(optimizer=optimizers.RMSprop(lr=2e-5), loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = resnet_model.fit(train_images, train_labels, epochs=20, batch_size=20,\n",
    "                    validation_data=(test_images, test_labels), callbacks=[callback1])\n",
    "\n",
    "\n",
    "#comment this out after you run it\n",
    "resnet_model.save('resnet.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save so we don't have to retrain everytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = tf.keras.models.load_model('resnet.h5')\n",
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = [];\n",
    "new_images = [];\n",
    "\n",
    "for j,i in enumerate(unlabelled_data.iterrows()):\n",
    "    if j < 500:\n",
    "        new_images.append(get_image(i[0]))\n",
    "#         print(i[0])\n",
    "\n",
    "\n",
    "predictions = resnet_model.predict(np.array(new_images))\n",
    "for i in predictions:\n",
    "    new_labels.append(get_label(i))\n",
    "    \n",
    "print(len(new_images), len(new_labels))\n",
    "\n",
    "new_train_images = np.concatenate((np.array(new_images), train_images), axis=0)\n",
    "\n",
    "#copy is original train_labels, n=2000, not encoded\n",
    "new_train_labels = np.concatenate((new_labels, copy), axis=0)\n",
    "new_train_labels = encode(new_train_labels)\n",
    "\n",
    "train_labels = np.array(new_train_labels[:2100])\n",
    "print(len(new_train_images),len(train_labels))\n",
    "print(len(test_labels), len(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resnet_model2 = tf.keras.models.Sequential()\n",
    "resnet_model2.add(layers.UpSampling2D((2,2), input_shape=(32,32,3)))\n",
    "resnet_model2.add(layers.UpSampling2D((2,2)))\n",
    "resnet_model2.add(layers.UpSampling2D((2,2)))\n",
    "resnet_model2.add(resnet)\n",
    "resnet_model2.add(layers.Flatten())\n",
    "resnet_model2.add(layers.BatchNormalization())\n",
    "resnet_model2.add(layers.Dense(128, activation='relu'))\n",
    "resnet_model2.add(layers.Dropout(0.5))\n",
    "resnet_model2.add(layers.BatchNormalization())\n",
    "resnet_model2.add(layers.Dense(64, activation='relu'))\n",
    "resnet_model2.add(layers.Dropout(0.5))\n",
    "resnet_model2.add(layers.BatchNormalization())\n",
    "resnet_model2.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "resnet_model2.compile(optimizer=optimizers.RMSprop(lr=2e-5), loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = resnet_model2.fit(new_train_images, train_labels, epochs=20, batch_size=20,\n",
    "                    validation_data=(test_images, test_labels), callbacks=[callback1])\n",
    "\n",
    "#comment this out after you run it\n",
    "resnet_model2.save('resnet2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resnet_model2 = tf.keras.models.load_model('resnet.h5')\n",
    "resnet_model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = []\n",
    "for i in range(1000):\n",
    "    test.append(get_test_image(i))\n",
    "\n",
    "    \n",
    "# predictions = new_model.predict(np.array(test))\n",
    "predictions = resnet_model2.predict(np.array(test))\n",
    "\n",
    "l = []\n",
    "for i in predictions:\n",
    "    l.append(get_label(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write predictions to csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"submission.csv\",\"w+\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for i,j in enumerate(l):\n",
    "        writer.writerow([f\"{i}.png\", f\"{j}\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
