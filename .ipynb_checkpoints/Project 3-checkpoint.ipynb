{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 for CSE142\n",
    "## CIFAR-10 Classification\n",
    "\n",
    "Importing necessary packages:\n",
    "\n",
    "- tensorflow/keras: API to build and train models\n",
    "- matplotlib: generate graphs\n",
    "- numpy: perform array operations\n",
    "- pandas: to work with dataframes\n",
    "- pickle: for serializing and de-serializing a Python object\n",
    "- os: for file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import pickle\n",
    "# import os, os.path\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Initialize relevant variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv(\"Semi1_Labels.csv\") #change this for different versions\n",
    "train_path = 'Train_Image' \n",
    "test_path = 'Test_Image' \n",
    "labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(img_number):\n",
    "    img = mpimg.imread(train_path + f\"/{img_number}.png\") \n",
    "    return img\n",
    "def get_test_image(img_number):\n",
    "    img = mpimg.imread(test_path + f\"/{img_number}.png\") \n",
    "    return img\n",
    "def get_label(arr):\n",
    "    max_index_col = np.argmax(arr, axis=0)\n",
    "    return labels[max_index_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(get_image(7777))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(get_test_image(999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function that prints image info**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_info(img):\n",
    "    rows,cols,channels = img.shape \n",
    "    img_size = rows*cols*channels # channels means rgb colors i think, so should always be 3?\n",
    "    img_to_1D = img.reshape(img_size) #numPy reshape\n",
    "    print(\"rows, cols, channels:\", rows, cols, channels)\n",
    "    print(\"image size:\", img_size)\n",
    "#     print(\"image original:\", img)\n",
    "#     print(\"image reshape:\", img_to_1D)\n",
    "#     return img_to_1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_img_info(get_image(3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the  Data\n",
    "\n",
    "We need to separate the labeled data from the unlabeled\n",
    "\n",
    "Also need to encode labels as numbers instead of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is_unlabelled = labels_df[\"Train Label\"] == NAN\n",
    "labelled = (labels_df[\"Train Label\"]!= \"NAN\")\n",
    "unlabelled = (labels_df[\"Train Label\"] == \"NAN\")\n",
    "\n",
    "unlabelled_data = (labels_df[unlabelled])\n",
    "print(unlabelled_data)\n",
    "\n",
    "labelled_data = (labels_df[labelled])\n",
    "print(labelled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = []\n",
    "unlabeled = []\n",
    "\n",
    "labeled = labelled_data[\"Train Label\"].tolist()\n",
    "unlabeled = unlabelled_data[\"Train Label\"].tolist()\n",
    "\n",
    "# print (labeled) \n",
    "# print (unlabeled) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#col = labels_df.columns\n",
    "#for i, entry in enumerate(labels_df[col[0]]):\n",
    " #   if (entry == 'NAN'):\n",
    "  #      unlabeled.append(i)\n",
    "   # else:\n",
    "   #     labeled.append([i, entry])\n",
    "# print(\"labeled\", labeled)\n",
    "# print(\"unlabeled\", unlabeled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to reshape training data and labels\n",
    "train_images, train_labels, test_images, test_labels = [], [], [], []\n",
    "for i in labelled_data.iterrows():\n",
    "    train_images.append(get_image(i[0]))\n",
    "    train_labels.append(i[1]['Train Label'])\n",
    "print(len(train_images))\n",
    "for i in range(36):\n",
    "    plt.subplot(6,6,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i])\n",
    "    plt.xlabel(train_labels[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change this to one-hot\n",
    "for i in range(len(train_labels)):\n",
    "    for j in range(len(labels)):\n",
    "        if train_labels[i] == labels[j]:\n",
    "            train_labels[i] = j\n",
    "# print (train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split labeled data so that we have some labeled testing data 80/20\n",
    "test_images =  np.array(train_images[1600:])\n",
    "train_images = np.array(train_images[:1600])\n",
    "test_labels = np.array(train_labels[1600:])\n",
    "train_labels = np.array(train_labels[:1600])\n",
    "print(len(train_images), len(train_labels), len(test_images), len(test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "### Convolutional Neural Network\n",
    "\n",
    "As input, a CNN takes tensors of shape (height, width, channels) = (32, 32, 3)\n",
    "\n",
    "This is based on https://www.tensorflow.org/tutorials/images/cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Is this stopping too soon?\n",
    "callback1 = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=4, restore_best_weights=True)\n",
    "callback2 = tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0, patience=3, restore_best_weights=True)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy']) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_images, train_labels, epochs=100, \n",
    "                    validation_data=(test_images, test_labels), callbacks=[callback2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label some of our unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = [];\n",
    "new_images = [];\n",
    "\n",
    "for j,i in enumerate(unlabelled_data.iterrows()):\n",
    "    if j < 500:\n",
    "        new_images.append(get_image(i[0]))\n",
    "#         print(i[0])\n",
    "\n",
    "\n",
    "predictions = model.predict(np.array(new_images))\n",
    "for i in predictions:\n",
    "    new_labels.append(get_label(i))\n",
    "    \n",
    "print(len(new_images), len(new_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.grid(False)\n",
    "    plt.imshow(new_images[i])\n",
    "    plt.xlabel(new_labels[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_train_images = np.concatenate((train_images, new_images), axis=0)\n",
    "new_train_labels = np.concatenate((train_labels, new_labels), axis=0)\n",
    "\n",
    "for i in range(len(new_train_labels)):\n",
    "    for j in range(len(labels)):\n",
    "        if new_train_labels[i] == labels[j]:\n",
    "            new_train_labels[i] = j\n",
    "            \n",
    "new_train_labels = tf.strings.to_number(new_train_labels)\n",
    "            \n",
    "print(len(new_train_images),len(new_train_labels))\n",
    "\n",
    "print(new_train_labels, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = models.Sequential()\n",
    "model2.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "model2.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "model2.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(layers.Dense(64, activation='relu'))\n",
    "model2.add(layers.Dense(10))\n",
    "model2.summary()\n",
    "\n",
    "model2.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy']) \n",
    "\n",
    "history2 = model2.fit(new_train_images, new_train_labels, epochs=100, \n",
    "                    validation_data=(test_images, test_labels), callbacks=[callback1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet 50 \n",
    "\n",
    "As per TA recommendation\n",
    "\n",
    "## TODO: figure this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(128,128,3), pooling='max')\n",
    "resnet = ResNet50(include_top=False, input_shape=(128,128,3), pooling='max')\n",
    "\n",
    "for layer in resnet.layers:\n",
    "#     print(layer)\n",
    "    layer.trainable = False\n",
    "\n",
    "    \n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "normal = tf.keras.layers.BatchNormalization()\n",
    "dropout =tf.keras.layers.Dropout(0.4)\n",
    "output = tf.keras.layers.Dense(10, activation='softmax')(resnet.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tf.keras.models.Model(resnet.input, output)\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_model = tf.keras.models.Sequential()\n",
    "augmented_model.add(tf.keras.layers.Conv2DTranspose(3, (3, 3), strides=2, padding='same', activation='relu', input_shape=(32,32,3)))\n",
    "augmented_model.add(tf.keras.layers.BatchNormalization())\n",
    "augmented_model.add(tf.keras.layers.Conv2DTranspose(3, (3, 3), strides=2, padding='same', activation='relu'))\n",
    "augmented_model.add(tf.keras.layers.BatchNormalization())\n",
    "augmented_model.add(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_model.compile(optimizer=Adam(lr=0.001),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = augmented_model.fit(train_images, train_labels, epochs=20, batch_size=32, verbose=1,\n",
    "                    validation_data=(test_images, test_labels), callbacks=[callback1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_model2 = tf.keras.models.Sequential()\n",
    "augmented_model2.add(tf.keras.layers.Conv2DTranspose(3, (3, 3), strides=2, padding='same', activation='relu', input_shape=(32,32,3)))\n",
    "augmented_model2.add(tf.keras.layers.BatchNormalization())\n",
    "augmented_model2.add(tf.keras.layers.Conv2DTranspose(3, (3, 3), strides=2, padding='same', activation='relu'))\n",
    "augmented_model2.add(tf.keras.layers.BatchNormalization())\n",
    "augmented_model2.add(res)\n",
    "\n",
    "augmented_model2.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = augmented_model2.fit(new_train_images, new_train_labels, epochs=100, \n",
    "                    validation_data=(test_images, test_labels), callbacks=[callback1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "\n",
    "Using our model and ResNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write predictions to csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "f = open(\"submission.csv\",\"w+\")\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
